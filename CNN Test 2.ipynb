{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.15"},"colab":{"name":"CNN Test 2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"sYc64Obz5hOB","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import tensorflow as tf\n","from tensorflow.keras import Model, layers\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBumZhVA5hOH","colab_type":"code","colab":{}},"source":["# MNIST dataset parameters.\n","num_classes = 10 # total classes (0-9 digits).\n","num_features = 784 # data features (img shape: 28*28).\n","\n","# Training parameters.\n","learning_rate = 0.1\n","training_steps = 2000\n","batch_size = 256\n","display_step = 100\n","\n","# Network parameters.\n","n_hidden_1 = 128 # 1st layer number of neurons.\n","n_hidden_2 = 256 # 2nd layer number of neurons."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"28PvgvdO5hOM","colab_type":"code","outputId":"302dff41-babe-41e4-8419-80bbd7299007","executionInfo":{"status":"ok","timestamp":1589460966412,"user_tz":-60,"elapsed":2768,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["# Prepare MNIST data.\n","from tensorflow.keras.datasets import mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","# Convert to float32.\n","x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n","# Flatten images to 1-D vector of 784 features (28*28).\n","x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n","# Normalize images value from [0, 255] to [0, 1].\n","x_train, x_test = x_train / 255., x_test / 255."],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RRwbEFUu5hOS","colab_type":"code","colab":{}},"source":["# Use tf.data API to shuffle and batch data.\n","train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dko8BNr05hOW","colab_type":"code","colab":{}},"source":["# Create TF Model.\n","class NeuralNet(Model):\n","    # Set layers.\n","    def __init__(self):\n","        super(NeuralNet, self).__init__()\n","        # First fully-connected hidden layer.\n","        self.fc1 = layers.Dense(n_hidden_1, activation=tf.nn.relu)\n","        # First fully-connected hidden layer.\n","        self.fc2 = layers.Dense(n_hidden_2, activation=tf.nn.relu)\n","        # Second fully-connecter hidden layer.\n","        self.out = layers.Dense(num_classes, activation=tf.nn.softmax)\n","\n","    # Set forward pass.\n","    def call(self, x, is_training=False):\n","        x = self.fc1(x)\n","        x = self.out(x)\n","        if not is_training:\n","            # tf cross entropy expect logits without softmax, so only\n","            # apply softmax when not training.\n","            x = tf.nn.softmax(x)\n","        return x\n","\n","# Build neural network model.\n","neural_net = NeuralNet()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TeKl9Ob5hOa","colab_type":"code","colab":{}},"source":["# Cross-Entropy Loss.\n","# Note that this will apply 'softmax' to the logits.\n","def cross_entropy_loss(x, y):\n","    # Convert labels to int 64 for tf cross-entropy function.\n","    y = tf.cast(y, tf.int64)\n","    # Apply softmax to logits and compute cross-entropy.\n","    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n","    # Average loss across the batch.\n","    return tf.reduce_mean(loss)\n","\n","# Accuracy metric.\n","def accuracy(y_pred, y_true):\n","    # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n","    correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n","    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n","\n","# Stochastic gradient descent optimizer.\n","optimizer = tf.optimizers.SGD(learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wDFN1-X5hOd","colab_type":"code","colab":{}},"source":["# Optimization process. \n","def run_optimization(x, y):\n","    # Wrap computation inside a GradientTape for automatic differentiation.\n","    with tf.GradientTape() as g:\n","        # Forward pass.\n","        pred = neural_net(x, is_training=True)\n","        # Compute loss.\n","        loss = cross_entropy_loss(pred, y)\n","        \n","    # Variables to update, i.e. trainable variables.\n","    trainable_variables = neural_net.trainable_variables\n","\n","    # Compute gradients.\n","    gradients = g.gradient(loss, trainable_variables)\n","    \n","    # Update W and b following gradients.\n","    optimizer.apply_gradients(zip(gradients, trainable_variables))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"taOcK3Uf5hOh","colab_type":"code","outputId":"dc0362d5-f195-4b2c-d28a-508cfbb08abc","executionInfo":{"status":"ok","timestamp":1589460984354,"user_tz":-60,"elapsed":20647,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"colab":{"base_uri":"https://localhost:8080/","height":381}},"source":["# Run training for the given number of steps.\n","for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n","    # Run the optimization to update W and b values.\n","    run_optimization(batch_x, batch_y)\n","    \n","    if step % display_step == 0:\n","        pred = neural_net(batch_x, is_training=True)\n","        loss = cross_entropy_loss(pred, batch_y)\n","        acc = accuracy(pred, batch_y)\n","        print(\"step: %i, loss: %f, accuracy: %f\" % (step, loss, acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["step: 100, loss: 2.041366, accuracy: 0.511719\n","step: 200, loss: 1.889186, accuracy: 0.632812\n","step: 300, loss: 1.815806, accuracy: 0.707031\n","step: 400, loss: 1.767255, accuracy: 0.738281\n","step: 500, loss: 1.689750, accuracy: 0.804688\n","step: 600, loss: 1.645507, accuracy: 0.855469\n","step: 700, loss: 1.617739, accuracy: 0.882812\n","step: 800, loss: 1.665235, accuracy: 0.847656\n","step: 900, loss: 1.618037, accuracy: 0.886719\n","step: 1000, loss: 1.607418, accuracy: 0.886719\n","step: 1100, loss: 1.604937, accuracy: 0.898438\n","step: 1200, loss: 1.578148, accuracy: 0.914062\n","step: 1300, loss: 1.586311, accuracy: 0.910156\n","step: 1400, loss: 1.582020, accuracy: 0.902344\n","step: 1500, loss: 1.567502, accuracy: 0.910156\n","step: 1600, loss: 1.581110, accuracy: 0.890625\n","step: 1700, loss: 1.566384, accuracy: 0.917969\n","step: 1800, loss: 1.584205, accuracy: 0.898438\n","step: 1900, loss: 1.544020, accuracy: 0.945312\n","step: 2000, loss: 1.554803, accuracy: 0.921875\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8HYjnzTJ5hOm","colab_type":"code","outputId":"b103c865-b10f-459e-842f-118f96fab6cc","executionInfo":{"status":"ok","timestamp":1589460984356,"user_tz":-60,"elapsed":20636,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Test model on validation set.\n","pred = neural_net(x_test, is_training=False)\n","print(\"Test Accuracy: %f\" % accuracy(pred, y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Accuracy: 0.919200\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pJiTrVsP5hOp","colab_type":"code","colab":{}},"source":["# Visualize predictions.\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import cv2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y8axRwW05hOr","colab_type":"code","colab":{}},"source":["\n","n_images =1\n","import cv2\n","img = Image.open(\"/content/imageTest.png\")\n","img_array = np.asarray(img)\n","resized = cv2.resize(img_array, (28, 28 ))\n","gray_scale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY) #(28, 28)\n","image = cv2.bitwise_not(gray_scale)\n","\n","image = image / 255\n","img = image.reshape(1, 784)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jkhs137uC9Lr","colab_type":"code","outputId":"54369717-52d5-45af-a923-4593dfdd17b2","executionInfo":{"status":"ok","timestamp":1589461482734,"user_tz":-60,"elapsed":537,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"colab":{"base_uri":"https://localhost:8080/","height":489}},"source":["\n","predictions = neural_net(img)\n","print(predictions)\n","# Display image and model prediction.\n","for i in range(n_images):\n","    plt.imshow(np.reshape(img, [28, 28]), cmap='gray')\n","    plt.show()\n","    print(\"Model prediction: %i\" % np.argmax(predictions.numpy()[i]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0514 13:04:42.307241 140139219883904 base_layer.py:1790] Layer neural_net is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["tf.Tensor(\n","[[0.09224133 0.08804105 0.17954886 0.08809983 0.08804105 0.11161086\n","  0.08804111 0.08804105 0.08829386 0.08804105]], shape=(1, 10), dtype=float32)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["Model prediction: 2\n"],"name":"stdout"}]}]}