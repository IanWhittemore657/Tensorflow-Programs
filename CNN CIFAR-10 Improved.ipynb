{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN CIFAR-10 Improved.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNI9zHKZ0M0mGL40/ejX0sr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"J5Q2dqdSUi6p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597784826354,"user_tz":-60,"elapsed":4501,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"outputId":"82443455-b9db-43fe-b2d8-0fa679a28d76"},"source":["import tensorflow as tf\n","print(tf.__version__)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, MaxPooling2D, BatchNormalization\n","from tensorflow.keras.models import Model"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NQcXuwyfUqnl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1597784830231,"user_tz":-60,"elapsed":8285,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"outputId":"a67ebd51-9031-4fce-a089-4508837e24dd"},"source":["# Load in the data\n","cifar10 = tf.keras.datasets.cifar10\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","y_train, y_test = y_train.flatten(), y_test.flatten()\n","print(\"x_train.shape:\", x_train.shape)\n","print(\"y_train.shape\", y_train.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 4s 0us/step\n","x_train.shape: (50000, 32, 32, 3)\n","y_train.shape (50000,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7oAvuGcmHW0U","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597784830233,"user_tz":-60,"elapsed":8282,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"TQql3K4XUsyA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597784830235,"user_tz":-60,"elapsed":8272,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}},"outputId":"dd376a0e-2bab-46bd-f8f0-1c5a94c4047b"},"source":["# number of classes\n","K = len(set(y_train))\n","print(\"number of classes:\", K)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["number of classes: 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sIzCyAvyUvoY","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597784836960,"user_tz":-60,"elapsed":14994,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}}},"source":["# Build the model using the functional API ( the commented out sections are the old version of CIFAR model)\n","i = Input(shape=x_train[0].shape)\n","\n","#removing the striding and using maxpooling2D works better for smaller images , this is different from the VGG as they use 5 groups of conv layers with multiple conv inside each \n","\n","# x = Conv2D(32, (3, 3), strides=2, activation='relu')(i)\n","# x = Conv2D(64, (3, 3), strides=2, activation='relu')(x)\n","# x = Conv2D(128, (3, 3), strides=2, activation='relu')(x)\n","\n","#the padding is used because without it the image would shrink after each conv which will make the image too small for it work with so many conv layers\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\n","#Adding a batch normalization that will help with overfitting\n","x = BatchNormalization()(x)\n","\n","x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n","x = BatchNormalization()(x)\n","\n","x = MaxPooling2D((2, 2))(x)\n","#tried using dropout to see what would happen to results but it could break the pattern for the image recognition\n","# x = Dropout(0.2)(x)\n","\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = BatchNormalization()(x)\n","\n","x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n","x = BatchNormalization()(x)\n","\n","x = MaxPooling2D((2, 2))(x)\n","# x = Dropout(0.2)(x)\n","\n","x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n","x = BatchNormalization()(x)\n","\n","x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n","x = BatchNormalization()(x)\n","\n","x = MaxPooling2D((2, 2))(x)\n","\n","# x = Dropout(0.2)(x)\n","# x = GlobalMaxPooling2D()(x)\n","\n","#this is the same as the last one \n","x = Flatten()(x)\n","x = Dropout(0.2)(x)\n","x = Dense(1024, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","x = Dense(K, activation='softmax')(x)\n","\n","model = Model(i, x)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xUcMifziU8vU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597784836965,"user_tz":-60,"elapsed":14995,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}}},"source":["# Compile\n","#make sure you are using the GPU runtime as this will take a long time\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"pjwvWPFNU-wL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597784836967,"user_tz":-60,"elapsed":14995,"user":{"displayName":"Ian W","photoUrl":"","userId":"01276571831026620870"}}},"source":["# Fit\n","#r = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ouZa8PdWVBK1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":663},"outputId":"c7248d72-34a3-4cc0-ffc2-32a84c1a9623"},"source":["# Fit with data augmentation\n","#RESET the run times and only run this part to see results with no trained weights\n","\n","#otherwise it will continue to train with the weights declared (This can be used for fine tuning)\n","#run this after the code above to see the difference though with fine tuning \n","\n","batch_size = 32\n","\n","#this is used to move the images around to train it better \n","data_generator = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n","\n","train_generator = data_generator.flow(x_train, y_train, batch_size)\n","\n","steps_per_epoch = x_train.shape[0] // batch_size\n","\n","\n","r = model.fit(train_generator, validation_data=(x_test, y_test), steps_per_epoch=steps_per_epoch, epochs=50)\n","\n","#the higher the val accuracy means less likely to overfit "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/50\n","1562/1562 [==============================] - 28s 18ms/step - loss: 1.4378 - accuracy: 0.4991 - val_loss: 1.1351 - val_accuracy: 0.5942\n","Epoch 2/50\n","1562/1562 [==============================] - 28s 18ms/step - loss: 0.9780 - accuracy: 0.6603 - val_loss: 0.9990 - val_accuracy: 0.6557\n","Epoch 3/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.8375 - accuracy: 0.7109 - val_loss: 0.7702 - val_accuracy: 0.7375\n","Epoch 4/50\n","1562/1562 [==============================] - 27s 18ms/step - loss: 0.7433 - accuracy: 0.7457 - val_loss: 0.7309 - val_accuracy: 0.7544\n","Epoch 5/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.6922 - accuracy: 0.7651 - val_loss: 0.6427 - val_accuracy: 0.7851\n","Epoch 6/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.6353 - accuracy: 0.7834 - val_loss: 0.6849 - val_accuracy: 0.7769\n","Epoch 7/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.5926 - accuracy: 0.7985 - val_loss: 0.7629 - val_accuracy: 0.7466\n","Epoch 8/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.5521 - accuracy: 0.8122 - val_loss: 0.5653 - val_accuracy: 0.8075\n","Epoch 9/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.5194 - accuracy: 0.8236 - val_loss: 0.6366 - val_accuracy: 0.7917\n","Epoch 10/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.4898 - accuracy: 0.8327 - val_loss: 0.5115 - val_accuracy: 0.8329\n","Epoch 11/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.4658 - accuracy: 0.8421 - val_loss: 0.5233 - val_accuracy: 0.8251\n","Epoch 12/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.4420 - accuracy: 0.8497 - val_loss: 0.5441 - val_accuracy: 0.8216\n","Epoch 13/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.4266 - accuracy: 0.8537 - val_loss: 0.4439 - val_accuracy: 0.8554\n","Epoch 14/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.4019 - accuracy: 0.8625 - val_loss: 0.4479 - val_accuracy: 0.8491\n","Epoch 15/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.3835 - accuracy: 0.8680 - val_loss: 0.4704 - val_accuracy: 0.8502\n","Epoch 16/50\n","1562/1562 [==============================] - 26s 17ms/step - loss: 0.3726 - accuracy: 0.8724 - val_loss: 0.4125 - val_accuracy: 0.8622\n","Epoch 17/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.3667 - accuracy: 0.8733 - val_loss: 0.4373 - val_accuracy: 0.8512\n","Epoch 18/50\n","1562/1562 [==============================] - 27s 17ms/step - loss: 0.3485 - accuracy: 0.8799 - val_loss: 0.4479 - val_accuracy: 0.8555\n","Epoch 19/50\n"," 762/1562 [=============>................] - ETA: 13s - loss: 0.3315 - accuracy: 0.8865"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nXZOGB37VIHm","colab_type":"code","colab":{}},"source":["# Plot loss per iteration\n","import matplotlib.pyplot as plt\n","plt.plot(r.history['loss'], label='loss')\n","plt.plot(r.history['val_loss'], label='val_loss')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yjrCIXt-VKjr","colab_type":"code","colab":{}},"source":["# Plot accuracy per iteration\n","plt.plot(r.history['accuracy'], label='acc')\n","plt.plot(r.history['val_accuracy'], label='val_acc')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldXvd84vVMad","colab_type":"code","colab":{}},"source":["# Plot confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","  \"\"\"\n","  This function prints and plots the confusion matrix.\n","  Normalization can be applied by setting `normalize=True`.\n","  \"\"\"\n","  if normalize:\n","      cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","      print(\"Normalized confusion matrix\")\n","  else:\n","      print('Confusion matrix, without normalization')\n","\n","  print(cm)\n","\n","  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","  plt.title(title)\n","  plt.colorbar()\n","  tick_marks = np.arange(len(classes))\n","  plt.xticks(tick_marks, classes, rotation=45)\n","  plt.yticks(tick_marks, classes)\n","\n","  fmt = '.2f' if normalize else 'd'\n","  thresh = cm.max() / 2.\n","  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","      plt.text(j, i, format(cm[i, j], fmt),\n","               horizontalalignment=\"center\",\n","               color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","  plt.tight_layout()\n","  plt.ylabel('True label')\n","  plt.xlabel('Predicted label')\n","  plt.show()\n","\n","\n","p_test = model.predict(x_test).argmax(axis=1)\n","cm = confusion_matrix(y_test, p_test)\n","plot_confusion_matrix(cm, list(range(10)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42Q-OU4-VOJY","colab_type":"code","colab":{}},"source":["# label mapping\n","labels = '''Moth\n","Noise'''.split()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvI3i7WSVPko","colab_type":"code","colab":{}},"source":["# Show misclassified examples\n","misclassified_idx = np.where(p_test != y_test)[0]\n","i = np.random.choice(misclassified_idx)\n","plt.imshow(x_test[i], cmap='gray')\n","plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aml2ckVFVP75","colab_type":"code","colab":{}},"source":["# Show classified examples\n","classified = np.where(p_test == y_test)[0]\n","i = np.random.choice(classified)\n","plt.imshow(x_test[i], cmap='gray')\n","plt.title(\"True label: %s Predicted: %s\" % (labels[y_test[i]], labels[p_test[i]]));"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNFoKVYEVZjd","colab_type":"code","colab":{}},"source":["# Since the model that was created was quite large, it is useful to summarise the model\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bGutG5-xh6hh","colab_type":"text"},"source":["# **EPOCH**\n","\n","---\n","\n","\n","**Orginial CIFAR EPOCH**\n","\n","![Normal](https://drive.google.com/uc?id=1yLGON7U7jT_2ByI9i7hajq-k5dWogS8H)\n","\n","(loss 0.8424 - Acc 0.7012 - val_loss 0.8657 - val_acc 0.7003) \n","\n","\n","---\n","\n","\n","**Without Batch normalisation but different model (Data augmentation)**\n","\n","![alt text](https://drive.google.com/uc?id=1RMSnQpBGccHP0GIATQdk6qbkduCWg0mo)\n","\n","(loss 0.0427 - Acc 0.9867 - val_loss 0.8767 - val_acc 0.8394) \n","\n","---\n","\n","**With only batch normalisation (Data Augmentation)**\n","\n","![alt text](https://drive.google.com/uc?id=1paJ2_IY4TgyBYPI5ZP-pSqBvJTnbYqmt)\n","\n","(loss 0.1837 - Acc 0.9376 - val_loss 0.4224 - val_acc 0.8755) \n","\n","---\n","\n","\n","**Fine tuning (First model run and the batch normalisation)**\n","\n","![alt text](https://drive.google.com/uc?id=1wn6ArKLGkH7BqbCTL_g6QlYdpAkllo9j)\n","\n","(loss 0.1608 - Acc 0.9461 - val_loss 0.4111 - val_acc 0.8845) \n","\n","---\n","\n","**Conclusion** \n","While the  new model without batch norm has a high accuracy, it is still not as good as the model using only batch which has slighlty less accuracy but much better val loss and val acc, and this can be improved on by using both models.fit\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k76C0wZDqOIl","colab_type":"text"},"source":["# **LOSS**\n","\n","**OLD CIFAR**\n","\n","![alt text](https://drive.google.com/uc?id=1z2Z51-MVcljClnyMbafSIy6VBXAU_ETx)\n","\n","\n","---\n","\n","**NEW MODEL WITH ONLY BATCH NORM**\n","\n","![alt text](https://drive.google.com/uc?id=1x5hSEz7Uqd5dnuBa5heTWEnUOMil_a_z)\n","\n","---\n","\n","**NEW MODEL WITH FINE TUNING**\n","\n","\n","![alt text](https://drive.google.com/uc?id=1JLSQeTQPfMkiF735uO9G_FrLKpZZf1UK)\n","\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g3beW-lSrDaD","colab_type":"text"},"source":["# ACCURACY\n","\n","**OLD CIFAR**\n","\n","![alt text](https://drive.google.com/uc?id=1ozzwbVT8a2Q9TwZX5Jkwdf6Sv3q_zY2l)\n","\n","---\n","\n","**NEW MODEL WITH BATCH NORM**\n","\n","\n","![alt text](https://drive.google.com/uc?id=1UYkVvIqIVFPJxi_23G7lV0TmGCLXQd3W)\n","\n","\n","\n","---\n","\n","**FINE TUNING **\n","\n","![alt text](https://drive.google.com/uc?id=13_rFmDyWy7ii0U1auvWmzAPls4daoOnt)"]},{"cell_type":"markdown","metadata":{"id":"WEkhQQ1Irthk","colab_type":"text"},"source":["# SCATTER MATRIX\n","\n","\n","**OLD CIFAR MODEL**\n","\n","![alt text](https://drive.google.com/uc?id=1Jp41TFsVkcceeCVcvt14gKQ-RXxpst48)\n","\n","---\n","\n","**NEW MODEL WITH ONLY BATCH NORM**\n","\n","![alt text](https://drive.google.com/uc?id=1-CVht8Na8w4gssWerLHK_asa2y1vsAII)\n","\n","---\n","\n","**FINE TUNING**\n","\n","![alt text](https://drive.google.com/uc?id=12N2S6yalNJcqgQdVVHDFYLcysw9m4dzb)\n"]},{"cell_type":"code","metadata":{"id":"B7PRL42QsH57","colab_type":"code","colab":{}},"source":["#THE CONFUSION MATRIX SHOWS HOW THE NUMBERS ARE MUCH LESS THAN THE ORGINAL MEANING THERE WILL BE LESS ERRORS AND BETTER RESULTS"],"execution_count":null,"outputs":[]}]}